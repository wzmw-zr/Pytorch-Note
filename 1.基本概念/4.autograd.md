# 自动求导系统(autograd)

Pytorch中实现了自动求导系统以支持计算图，体现在`torch.autograd`模块中。

## 一、`torch.autograd.backward()`

`torch.autograd.backward()`是**torch.backward()用来求取梯度的函数**。`torch.backward()`实际上就是调用了`torch.autograd.backward()`。

```python
def backward(
    tensors: _TensorOrTensors,
    grad_tensors: Optional[_TensorOrTensors] = None,
    retain_graph: Optional[bool] = None,
    create_graph: bool = False,
    grad_variables: Optional[_TensorOrTensors] = None,
) -> None
"""
tensors: 用于求导的张量，如Loss等。

grad_tensors: 向量值函数求导时指定，为了在计算Jaccobian矩阵，grad_tensors实际上就是权重矩阵，这种情况是对应多个Loss函数，最后求导时各个部分的权重。使用torch.backward()时，通过参数gradients指定。

retain_graph: 计算完导数之后是否保留计算图，Pytorch默认是不保存的。

create_graph: 创建导数的计算图，用于高阶求导
"""

"""
关于grad_tensors:
grad_tensors (sequence of (Tensor or None)) – The “vector” in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional.
"""
```



##  二、`torch.autograd.grad()`

`torch.autograd.grad()`用于计算计算图中一个点(张量)的梯度。

```python
def grad(
    outputs: _TensorOrTensors,
    inputs: _TensorOrTensors,
    grad_outputs: Optional[_TensorOrTensors] = None,
    retain_graph: Optional[bool] = None,
    create_graph: bool = False,
    only_inputs: bool = True,
    allow_unused: bool = False
) -> Tuple[torch.Tensor, ...]:
"""
outputs: 用于求导的张量，如Loss，对应计算图的输出。

inputs: 需要求梯度的张量。

grad_outputs: 同grad_tensors作用。

create_graph: 创建导数计算图，用于高阶求导。

retain_graph: 保存计算图。
"""
```



## 三、Pytorch自动求导机制的注意点

1. Pytorch反向传播计算了梯度之后，**叶子节点的梯度不会自动清零，而是会叠加。**
   + 当然，可以**手动调用`.zero_()`方法，清空`.grad`。**
   + **在`grad.nn`模块中，提供了清空神经网络叶子节点梯度方法。**因为机器学习，深度学习都是网络，这样子实现较为方便。
2. 依赖于叶子节点的节点，requires_grad默认为True。
3. 叶子节点不可以执行in_place。