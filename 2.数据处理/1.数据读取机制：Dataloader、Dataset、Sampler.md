# 数据读取机制：Dataset、Sampler、Dataloader

关于数据：

+ **数据收集**：例如收集图片img和对应标签Label，**组织成数据集，存储在磁盘上。**
+ **数据划分**：**磁盘上的数据按照用途，划分成训练集、验证集、测试集。**
+ **数据读取**：读入磁盘上需要的数据集的内容用于训练，涉及到`torch.utils.data.dataset, torch.utils.data.sampler，torch.utils.data.dataloader`等类。
+ **数据预处理**：计算机视觉使用`torchvision.transfroms`类对数据进行预处理(数据增强)。



## 一、Dataset—对数据集的抽象

torch.utils.data.dataset类是对**数据集的抽象**，是给dataloader加载数据的。我们定义的**训练数据集类、验证数据集类、测试数据集类**就是**继承**自torch.utils.data.dataset。

Pytorch支持两种类型的dataset：

+ **map-style datasets**：需要重写`__getitem__()`和`__len__()`方法，用于模拟容器类型(序列或映射)，这里是用来**支持下标访问和切片**。

  map-style datasets 可以通过`dataset[idx]`从磁盘读取`idx-th`图片及其对应label。(**具体的读数据操作还是要自己写的，也可以一开始就将数据全读进来。**)

  **==要使用map-style datasets，继承torch.utils.data.Dataset。==**

+ **interable-style datasets**：需要重写`__iter__()`，模拟容器类型，用于**迭代容器中的数据**。

  **==要使用iterable-style datasets，继承torch.utils.data.InterableDataset==**。

> 一般map-style datasets用的较多。

### 1. map-style datasets

```python
class Dataset(Generic[T_co]):
    """An abstract class representing a :class:`Dataset`.

    1. All datasets that represent a map from keys to data samples should subclass
       it. 
    2. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
       data sample for a given key. 
    3. Subclasses could also optionally overwrite :meth:`__len__`, 
       which is expected to return the size of the dataset by many
      :class:`~torch.utils.data.Sampler` implementations and the default options
       of :class:`~torch.utils.data.DataLoader`.

    .. note::
      :class:`~torch.utils.data.DataLoader` by default constructs a index
      sampler that yields integral indices.  To make it work with a map-style
      dataset with non-integral indices/keys, a custom sampler must be provided.
    """

    def __getitem__(self, index) -> T_co:
        raise NotImplementedError

    def __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]':
        return ConcatDataset([self, other])


    # No `def __len__(self)` default?
```

**==__len__()也需要重写，因为后面的sampler和dataloader会用到。==**

例子：

```python
# ================================================================== #
#                5. Input pipeline for custom dataset                 #
# ================================================================== #

# You should build your custom dataset as below.
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self):
        # TODO
        # 1. Initialize file paths or a list of file names. 
        pass
    def __getitem__(self, index):
        # TODO
        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).
        # 2. Preprocess the data (e.g. torchvision.Transform).
        # 3. Return a data pair (e.g. image and label).
        pass
    def __len__(self):
        # You should change 0 to the total size of your dataset.
        return 0 

# You can then use the prebuilt data loader. 
custom_dataset = CustomDataset()
train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,
                                           batch_size=64, 
                                           shuffle=True)
```



### 2. iterable-style datasets

为什么需要iterable-style datasets：

For [iterable-style datasets](https://pytorch.org/docs/stable/data.html#iterable-style-datasets), data loading order is entirely controlled by the user-defined iterable. This **allows easier implementations of chunk-reading and dynamic batch size** (e.g., by yielding a batched sample at each time).

要使用iterable-style datasets，重载`__iter__()`即可。

> 还有其他类型的的数据集类，不过map-style datasets在早期阶段足够使用了。



## 二、Sampler—对采样器的抽象

> Sampler的作用：**决定加载数据的顺序**。
>
> 因为interable-style datasets的访问顺序是被用户定下来了，所以Sampler只能对map-style dataset起作用。
>
> `torch.utils.data.Sampler`类的功能是指定data load过程中的indices或keys的序列，**==这里实际上代表就是数据集上索引的一个可迭代对象，按照需求采样，即产生数据集上的一些索引。==**

Sampler的使用：

+ 定义Dataloader时，通过**shuffle参数**指定采样器是**顺序采样还是随机采样**。

  > A sequential or shuffled sampler will be automatically constructed based on the `shuffle` argument to a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). 

+ **或者**，通过**sampler参数指定一个采样器(每次采样一个数据)**，该采样器能够每次产生后面要获取的数据的索引(或键)。

  > Alternatively, users may use the `sampler` argument to specify a custom [`Sampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler) object that at each time yields the next index/key to fetch.

+ 如果采样器每次产生批索引列表会(a list of batch indices)，**则需要通过batch_sampler参数指定该采样器。**Dataloader中可以再指定与批采样相关的参数，如batch_size, drop_last等。

Pytorch自己实现了常用的Sampler， 而我们也可以同继承`torch.utils.data.Sampler`自定义新sampler类来使用：

```python
class Sampler(Generic[T_co]):
    """Base class for all Samplers.

    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a
    way to iterate over indices of dataset elements, and a :meth:`__len__` method
    that returns the length of the returned iterators.

    .. note:: The :meth:`__len__` method isn't strictly required by
              :class:`~torch.utils.data.DataLoader`, but is expected in any
              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.
    """

    def __init__(self, data_source: Optional[Sized]) -> None:
        pass

    def __iter__(self) -> Iterator[T_co]:
        raise NotImplementedError
```

**自己实现的Sampler需要重载\_\_iter\_\_()方法去对dataset中的元素通过下标进行迭代。**

Pytorch中自己实现的常用采样器：

+ `torch.utils.data.SequentialSampler(data_source)`： data_source(Dataset)—dataset to sample from，返回的是单个索引值。

+ `torch.utils.data.RandomSampler()`：返回的是单个索引值。

+ `torch.utils.data.SubsetRandomSampler()`：返回的是单个索引值。

+ `torch.utils.data.WeightedRandomSampler()`：返回的是单个索引值。

+ `torch.utils.data.BatchSampler()`：返回的是一批索引值。

+ ``torch.utils.data.distributed.DistributedSampler()`。

  > 具体的参数看文档。





## 三、Dataloader—对数据获取过程的抽象

**Dataloader是对数据获取过程的抽象：==对数据集(Dataset)使用采样器(Sampler)获取数据==**。

```python
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
```

参数解释：

+ dataset：指定数据集。

+ batch_size: 一批数据的数量。

+ drop_out：批量采样最后剩余小于batch_size部分数据如何处理，drop_out=True，丢弃，否则保留。

+  shuffle：采样的数据是顺序采样还是随机采样，若shuffle=True，实际上会使用一个随机采样的采样器。

+ sampler：指定采样器。

+ batch_sampler：指定用于批数据采样的采样器(对map-style datasets可选，每次产生一个索引列表(或者键列表))。

+ num_workers：指定读取数据的进程数。

+ collate_fn: 将采样得到的数据整理成batched_data的自定义方法，默认是新建第一维进行整理。(**这里通常会用到`torch.stack()`方法。**)

  > A custom `collate_fn` can be used to **customize collation**, e.g., padding sequential data to max length of a batch. 



### 1. Dataloader Loading batched data

> **[`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) supports automatically collating individual fetched data samples into batches via arguments `batch_size`, `drop_last`, and `batch_sampler`**.

#### (1)  Automatic batching

Dataloader **==获取了一批数据之后通过`collate_fn`将它们整理成batched samples==**，batched samples 是Tensor，包含各个数据对应的Tensors，并且通常将第一维作为batch dimension (**即，在一个数据的shape上增加第一维，第一维用来堆数据，具体例子可见Linear Regression中自己生成的数据**)。

> This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).
>
> 1. When `batch_size` (default `1`) is not `None`, the data loader yields batched samples instead of individual samples. (**==只要batch_size不为None，数据都是batched_data, 即使batch_size=1也是生成batched_size。==**)
>
> 2. `batch_size` and `drop_last` arguments are used to specify how the data loader obtains batches of dataset keys. 
>
> 3. For map-style datasets, users can alternatively specify `batch_sampler`, which yields a list of keys at a time.
>
> 4. After fetching a list of samples using the indices from sampler, the function passed as the `collate_fn` argument is used to collate lists of samples into batches.
>
>    + In this case, **loading from a map-style dataset is roughly equivalent with**:
>
>    ```python
>    for indices in batch_sampler:
>        yield collate_fn([dataset[i] for i in indices])
>    ```
>
>    + and loading from an iterable-style dataset is roughly equivalent with:
>
>    ```python
>    dataset_iter = iter(dataset)
>    for indices in batch_sampler:
>        yield collate_fn([next(dataset_iter) for _ in indices])
>    ```



#### (2) Disable automatic batching

不开启自动批处理的场景：

1. 用户希望人工批处理数据集中的数据。
2. 仅仅是加载单独的数据，或者程序就是为了处理单个数据。
3. 直接加载batched data。
4. batch size与数据有关时。

此时，collect_fn的作用就仅仅是将numpy array转成Pytorch Tensors，其余不变，还是列表里存的数据，而不是维度扩展。

> **When automatic batching is disabled**, the default `collate_fn` simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.
>
> In this case, loading from a map-style dataset is roughly equivalent with:
>
> ```python
> for index in sampler:
>     yield collate_fn(dataset[index])
> ```
>
> and loading from an iterable-style dataset is roughly equivalent with:
>
> ```python
> for data in iter(dataset):
>     yield collate_fn(data)
> ```

