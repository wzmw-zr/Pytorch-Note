# 批标准化(Batch Normalization)

因为Internal Covariate Shift (ICS)，即数据尺度/分布异常，导致训练困难，所以才有了Batch Normalization及其变种。

## 一、批标准化

1. **批(Batch)**：一批数据，通常为mini-batch。

2. **标准化(Normalization)**：均值为0,方差为1。

3. **优点**：

   + 可以用更大的学习率，加速模型收敛。
   + 可以不用精心设计权值初始化。
   + 可以不用dropout或较小的dropout。
   + 可以不用L2或较小的weight decay。
   + 可以不用LRN。

   

## 二、算法

1. **Input**：

   + Values of x over a mini-batch: $B=\{x_1,...,x_m\}$;
   + Parameters to be learned: $\gamma,\beta$.

2. **output**: $\{y_i=\boldsymbol{BN}_{\gamma,\beta}(x_i)\}$.

3. **Step**:

   + $\mu_{B}\leftarrow \frac{1}{m}\sum\limits_{i=1}^m x_i$.  mini-batch mean.
   + $\sigma^2_{B}=\frac{1}{m}\sum\limits_{i=1}^m(x_i-\mu_B)^2$.  mini-batch variance.
   + $\hat{x_i}=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}$.  normalize, $\epsilon$ is a recitfied unit to prevent $\sigma^2_B=0$.
   + $y_i=\gamma \hat{x_i}+\beta \equiv \boldsymbol{BN}_{\gamma,\beta}(x_i)$.  scale and shift.

   

## 三、`nn.BatchNorm2d`类—二维Batch Normalization

```python
torch.nn.BatchNorm2d(
    num_features, 
    eps=1e-05, 
    momentum=0.1, 
    affine=True, 
    track_running_stats=True)

def __init__(
        self,
        num_features: int,
        eps: float = 1e-5,
        momentum: float = 0.1,
        affine: bool = True,
        track_running_stats: bool = True
    ) -> None:
        super(_NormBase, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats
        if self.affine:
            self.weight = Parameter(torch.Tensor(num_features))
            self.bias = Parameter(torch.Tensor(num_features))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
        if self.track_running_stats:
            self.register_buffer('running_mean', torch.zeros(num_features))
            self.register_buffer('running_var', torch.ones(num_features))
            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))
        else:
            self.register_parameter('running_mean', None)
            self.register_parameter('running_var', None)
            self.register_parameter('num_batches_tracked', None)
        self.reset_parameters()
```

**参数解释：**

+ num_features：一个样本的**特征数量**。**就是feature map的个数，或者说channel的个数。**
+ eps: 分布修正项
+ momentum：指数加权平均值估计当前的mean/var。
+ affine：是否要加affine transform。
+ track_running_stats：是训练状态还是测试状态。

**一些重要属性：**

+ running_mean：均值。
+ running_var：方差。
+ weight：affine transform中的$\gamma$。
+ bias：affine transform中的$\beta$。

$$
running\_mean = (1-momentum) \times pre\_running\_mean + momentum \times mean\_t\\
running\_var=(1-momentum) \times pre\_running\_var+momentum\times var\_t
$$

**Shape:**

- Input: (N, C, H, W)
- Output: (N, C, H, W) (same shape as input)

> BatchNorm1d, BatchNorm2d, BatchNorm3d都继承自_BatchNorm，输入是(N, C, n-D feature shape)。

```python
# With Learnable Parameters
m = nn.BatchNorm2d(100)
# Without Learnable Parameters
m = nn.BatchNorm2d(100, affine=False)
input = torch.randn(20, 100, 35, 45)
output = m(input)
```



## 二、其他常见的Normalization方法

除了BatchNormalization, 还有Layer Normalization， Instance Normalization,  Group Normalization。对应的公式可以看文档或论文。

Layer Normalization应用于变长的网络，是逐层计算均值和方差，不会再有running_mean, running_var，$\gamma,\beta$是逐元素的。

Instance Normalization：BN在图像生成(Image Generation)中不适用，因此选择了逐Instance(channel) 计算均值和方差。

Group Normaliztion：BN在大模型小batch样本中的估计值不准确，所以逐channel分组处理，不会再有running_mean, running_var。