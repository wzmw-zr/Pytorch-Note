# 学习率调整

在模型训练过程中我们会需要动态调整学习率，一开始的学习率较大，随着epoch的增大，学习率逐渐降低。

`torch.optim.lr_scheduler`提供了多个根据epoch对学习率进行调整的策略，看文档就行。

**学习率调整策略是在优化器更新过参数后进行**，因此，使用学习率调整策略的代码框架大致如下:

```python
scheduler = ...
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
```

以`torch.optim.lr_scheduler.LambdaLR`类为例：

```python
torch.optim.lr_scheduler.LambdaLR(
    optimizer, 
    lr_lambda, 
    last_epoch=-1, 
    verbose=False)
```

参数解释：

+ optimizer：优化器。
+ lr_lambda：根据epoch计算学习率的函数，也可以是存函数的列表。
+ last_epoch：最后一个epoch的索引，默认-1。
+ verbose：每次更新学习率后在标准输出中输出一些信息，默认为False。

具体的使用例子如下：

```python
# Assuming optimizer has two groups.
lambda1 = lambda epoch: epoch // 30
lambda2 = lambda epoch: 0.95 ** epoch
scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
```

换而言之，使用学习率调整策略，**实际上就是在优化器上再加上一层，在学习率调整策略中调用优化器。**







