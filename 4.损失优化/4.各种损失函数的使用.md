# 各种损失函数的使用

## 一、交叉熵损失函数(Cross Entropy Loss Function)

在Pytorch中，交叉熵损失函数是`nn.LogSoftmax()`和`nn.NLLLoss()`的组合。常用作**多分类问题的损失函数**。

Pytorch中的交叉熵**采用下面的形式**：
$$
H(p,q)=-\sum_{x}p(x)\log q(x)
$$
即**Softmax—log—NLLLoss**之后获得的结果。

> 二分类的话实际上等价于：
> $$
> H(p,q)=-\sum_{x}p(x)\log q(x)+(1-p(x))\log(1-q(x))
> $$

```python
torch.nn.NLLLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = 'mean')
"""
输入input(模型的输出)是(minibatch, C)或者(minibatch, C, d1, d2, ..., dk)[我也不知道这个应用场景。。。] k >= 1形状的tensor， C代表类别数量。
损失函数期望的target是[0, C - 1]的类的下标。
"""

"""
input: (N, C)，C代表类的数量， 或者(N, C, d1, d2, ..., dk)在损失函数是k维数据时。
target：(N)，其中每一个值都在[0, C - 1]或者(N, d1, d2, ..., dk)。
output：根据reduction判断，默认reduction = "mean"， 输出为常量。
"""
```

验证：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)

print(input)
print(loss(input, target))

soft_max_func = nn.Softmax(dim=1)
soft_output = soft_max_func(input)
print(soft_output)
soft_output = torch.log(soft_output)
nllloss_func = nn.NLLLoss()
nllloss_output = nllloss_func(soft_output, target)
print(nllloss_output)

input = F.softmax(input=input, dim=1)
print(input)
input = torch.log(input)
output = F.nll_loss(input, target)
print(output)
```



## 二、L1， L2(MSE)损失函数

```python
torch.nn.L1Loss(size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.MSELoss(size_average=None, reduce=None, reduction: str = 'mean')
"""
input: (N, *)
output： (N, *)， 形状和input一样
"""
```

