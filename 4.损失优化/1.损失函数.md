# 损失函数(Loss Function)

## 一、损失函数

1. 损失函数(Loss Function)：**衡量模型输出与真实标签的差异**。
   $$
   Loss=f(y,\hat{y})
   $$

2. 代价函数(Cost Function)：
   $$
   Cost=\frac{1}{N}\sum_{i=1}^Nf(y,\hat{y})
   $$
   **代价函数一般是在batch_size > 1时使用。**

3. 目标函数(Objective Function)：
   $$
   Obj=Cost+Regularization
   $$
   **目标函数才是实际上训练的最终优化目标，而目标函数必须基于代价函数、损失函数，加上正则化项来提高模型的泛化能力，减小过拟合。**

Pytorch中损失函数基类：

```python
class _Loss(Module):
    reduction: str

    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:
        super(_Loss, self).__init__()
        if size_average is not None or reduce is not None:
            self.reduction = _Reduction.legacy_get_string(size_average, reduce)
        else:
            self.reduction = reduction
```

现在参数只需要指定reduction就行。

**在Pytorch中，==损失函数也是Module==。**

## 二、L1、L2损失函数

L1损失函数：`nn.L1Loss`，$Loss(x,y)=\frac{1}{n}\sum_{i=1}^n|x_i-y_i|$。

L2损失函数：`nn.MSELoss`，$Loss(x,y)=\frac{1}{n}\sum_{i=1}^n(x+i-y_i)^2$。

> 关于reduction: str = 'mean'， 'sum'，其实每个数据的Loss求出来之后是取均值还是求和，一般取均值。

SmoothL1Loss：平滑的L1Loss。
$$
Loss(x,y)=\frac{1}{N}\sum_{i=1}^nz_i\\
z_i=
\left\{\begin{matrix}
0.5(x_i-y_i)^2\quad if \; |x_i-y_i|<1\\
|x_i-y_i|-0.5\quad otherwise
\end{matrix}
\right.
$$




## 三、`torch.nn.CrossEntropyLoss`类—交叉熵损失函数

1. **交叉熵**(Cross Entropy，CE)： 交叉熵源自信息论，用于**衡量两个分布的差异**，常用于**分类任务**中。
   $$
   H(p,q)=-\sum_{i=1}^np(x_i)\log(q(x_i))
   $$
   这里$p$是真实的分布，$q$是模型得到的分布，使用交叉熵进行优化目的在于尽可能**减小模型的分布与真实的分布之间的差异，让model的分布逼近真实的分布。**

2. **自信息**：描述某一个事件的**信息量**，$I(x)=-\log(P(x)),P(x)$是某事件发生的概率。

3. **信息熵**：描述信息的**不确定度**，所有可能取值的**信息量的期望**，即：
   $$
   H(x)=-\sum_{i=1}^Np_i\log(p_i)
   $$
   **信息熵越大，表示数据的不确定性越大**，否则，数据确定性越高。

4. **相对熵**：又称**K-L散度**，**衡量两个分布之间的差异**:
   $$
   D_{KL}(P||Q)=\sum_{i=1}^NP(x_i)(\log{P(X_i)-\log{Q(x_i)}})
   $$
   其中，P是真实分布，Q是model的分布。

   交叉熵、信息熵与相对熵之间的关系：

$$
H(p,q)=H(P)+D_{KL}(P||Q)
$$

> 交叉熵=信息熵+相对熵。因此，**==优化交叉熵就相当于优化相对熵==**，因为信息熵是常数。

5. 交叉熵要求预测是概率分布的形式，但是预测不一定是概率分布的形式：

   + 概率具有$0\le P(x)\le 1$，但是出现负数。

   + 概率和等于1，但是出现概率和不等于1。

     那么就需要**使用softmax函数，将数据变换到符合概率分布的形式。**

6. Softmax操作：

   + 取指数，实现非负。

   + 除以指数之和，实现和为1。

```python
torch.nn.CrossEntropyLoss(
    weight: Optional[torch.Tensor] = None, 
    size_average=None, 
    ignore_index: int = -100, 
    reduce=None, 
    reduction: str = 'mean')
```

**如果分类问题有C个类，那么weight就应当是各个类的权重组成的1-D Tensor，默认各个类是同权重的，都是1。**对于一个类而言，其
$$
loss(x,class)=weight[class]\times(-\log(\frac{\exp(x[class])}{\sum_{j}\exp(x[j])}))\\
这一步就使用了torch.nn.LogSoftmax(Softmax+Log)和torch.nn.NLLLoss(提供负号)
$$

$$
loss=\frac{\sum_{i=1}^Nloss(i,class[i])}{\sum_{i=1}^Nweight[class[i]]}
$$

使用：

```python
loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)
output.backward()
```

关于input、target与output形状：

- Input: (N, C) where C = number of classes, or (N, C, d_1, d_2, ..., d_K) with $K \ge1$ in the case of K-dimensional loss.
- Target: (N) where each value is 0 \leq \text{targets}[i] \leq C-10≤targets[*i*]≤*C*−1 , or (N, d_1, d_2, ..., d_K) with $K \geq 1$ in the case of K-dimensional loss.
- Output: scalar. If `reduction` is `'none'`, then the same size as the target: (N), or (N, d_1, d_2, ..., d_K) with $K \geq 1$in the case of K-dimensional loss.

> 即**==input第一维是batch_size==**，target是正确的分类结果，input中对应数据是预测结果，计算交叉熵。



## 四、`torch.nn.NLLLoss`类—The negative log likelihood loss

NLLLoss用于分类问题。

```python
torch.nn.NLLLoss(
    weight: Optional[torch.Tensor] = None, 
    size_average=None, 
    ignore_index: int = -100, 
    reduce=None, 
    reduction: str = 'mean')
```

NLLLoss计算时传入的**input包含的应当是每个类的概率的对数的(batch_size, Class_size)。**为了达到对数的效果，使用`nn.LogSoftmax`处理一遍就行。

target应当是分类结果，The target that this loss expects should be a class index in the range [0, C-1][0,*C*−1] where C = number of classes。
$$
Loss(x,y)=(l_1,...,l_N)^T\\
l_n=-w_nx_ny_n,w_c=weight[i]\times1(if\; c \neq target)
$$
input, target, output与CrossEntropyLoss相同。

```python
m = nn.LogSoftmax(dim=1)
loss = nn.NLLLoss()
# input is of size N x C = 3 x 5
input = torch.randn(3, 5, requires_grad=True)
# each element in target has to have 0 <= value < C
target = torch.tensor([1, 0, 4])
output = loss(m(input), target)
output.backward()


# 2D loss example (used, for example, with image inputs)
N, C = 5, 4
loss = nn.NLLLoss()
# input is of size N x C x height x width
data = torch.randn(N, 16, 10, 10)
conv = nn.Conv2d(16, C, (3, 3))
m = nn.LogSoftmax(dim=1)
# each element in target has to have 0 <= value < C
target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)
output = loss(m(conv(data)), target)
output.backward()
```



## 五、`nn.KLDivLoss`类—相对熵、KL散度

**相对熵**：又称**K-L散度**，**衡量两个分布之间的差异**:
$$
D_{KL}(P||Q)=\sum_{i=1}^NP(x_i)(\log{P(X_i)-\log{Q(x_i)}})
$$
其中，P是真实分布，Q是model的分布，**注意模型的输出进来之前就已经经过logSoftmax处理过了。**



## 六、`nn.MarginRankingLoss`

1. 功能：**==计算两个向量之间的相似度==，用于排序任务。**

   该方法计算两组数据之间的差异，返回一个$n\times n$的loss矩阵。

```python
torch.nn.MarginRankingLoss(margin: float = 0.0, size_average=None, reduce=None, reduction: str = 'mean')
"""
margin: 边界值

功能：
Creates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy (containing 1 or -1).

Shape:
Input1: (N) where N is the batch size.

Input2: (N) , same shape as the Input1.

Target: (N) , same shape as the inputs.

Output: scalar. If reduction is 'none', then (N).
"""
```

min-batch中的每个样本的损失函数：
$$
loss(x_1,x_2,y)=\max(0,-y\times(x_1-x_2)+margin)
$$
解释：

$y=1$时，希望$x_1$比$x_2$大，当$x_1>x_2$时，不产生loss。

$y=-1$，希望$x_2$比$x_1$大，当$x_2>x_1$时，不产生loss。



## 七、`nn.MultiLabelMarginLoss`—多标签边界损失函数

1. **功能**：用于多分类任务的margin-based loss，**图像分割、目标检测会用到。**

   + input： x   (a 2D mini-batch Tensor) 

   + output：y (which is a 2D Tensor of target class indices)
   + Shape:
     - Input: (C)or (N, C)where N is the batch size and C is the number of classes.
     - Target: (C)or (N, C), label targets padded by -1 ensuring same shape as the input.
     - Output: scalar. If `reduction` is `'none'`, then (N) .

```python
torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction: str = 'mean')
```

对min-batch中的每一个样本：
$$
loss(x,y)=\sum_{ij}\frac{\max(0,1-(x[y[j]]-x[i]))}{s.size(0)}\\
x\in \{0,...,x.size(0)-1\},y\in \{0,...,y.size(0)-1\}\\
0\le y[j]\le x.size(0)-1, \;\; \forall i,j\; i \neq y[j]
$$


> 还有很多损失函数，注意理解损失函数公式就可以了，**==后期要培养实现自定义损失函数的能力。==**

