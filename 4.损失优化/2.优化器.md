# 优化器(Optimizer)

## 一、优化器

1. Pytorch的优化器：**管理并更新**模型中可学习参数的值，即，在**==求出损失函数后反向传播进行求导，对参数进行优化==**，使得模型输出更接近真实标签。

```python
class Optimizer(object):
    """Base class for all optimizers.

    .. warning::
        Parameters need to be specified as collections that have a deterministic
        ordering that is consistent between runs. Examples of objects that don't
        satisfy those properties are sets and iterators over values of dictionaries.

    Arguments:
        params (iterable): an iterable of :class:`torch.Tensor` s or
            :class:`dict` s. Specifies what Tensors should be optimized.
        defaults: (dict): a dict containing default values of optimization
            options (used when a parameter group doesn't specify them).
    """

    def __init__(self, params, defaults):
        torch._C._log_api_usage_once("python.optimizer")
        self.defaults = defaults

        if isinstance(params, torch.Tensor):
            raise TypeError("params argument given to the optimizer should be "
                            "an iterable of Tensors or dicts, but got " +
                            torch.typename(params))

        self.state = defaultdict(dict)
        self.param_groups = []

        param_groups = list(params)
        if len(param_groups) == 0:
            raise ValueError("optimizer got an empty parameter list")
        if not isinstance(param_groups[0], dict):
            param_groups = [{'params': param_groups}]

        for param_group in param_groups:
            self.add_param_group(param_group)
```

Optimizer的主要属性：

+ defaults：**优化器的超参数**，必然有其用途，只是我还没碰到。。。
+ state：**参数的缓存**。
+ param_groups：**管理的参数组。** **param_groups是列表，存的是字典，每个字典的值就是各个要更新的参数组。**实例化优化器时，传入的是元素是Tensor或dict的可迭代对象，如果可迭代对象中是Tensor，那么这就是一个参数组，之后加入到self.param_groups中; 如果是dict，即一堆参数组，那么全加进param_groups中。
+ _step_count：记录更新次数，**学习率调整中使用**。

实例化优化器的例子：

```python
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# model.parameters()，模型的参数，调用很方便。
optimizer = optim.Adam([var1, var2], lr=0.0001)
```



基本方法：

1. `zero_grad()`：**清空所管理的参数的梯度**。因为Pytorch的张量梯度不会自动清零，需要我们手动清零，但是参数多了就不方便，所以就整体清零。

   ```python
   def zero_grad(self, set_to_none: bool = False):
           for group in self.param_groups:
               for p in group['params']:
                   if p.grad is not None:
                       if set_to_none:
                           p.grad = None
                       else:
                           if p.grad.grad_fn is not None:
                               p.grad.detach_()
                           else:
                               p.grad.requires_grad_(False)
                           p.grad.zero_()
   ```

2. `step()`：**执行一步更新**。

   + 简单实现的版本,**在每一次迭代训练最后进行参数更新。**

   ```python
   for input, target in dataset:
       optimizer.zero_grad()
       output = model(input)
       loss = loss_fn(output, target)
       loss.backward()
       optimizer.step()
   ```

   + 对于需要多次重新计算函数的优化算法，如Conjugate Gradient, LBFGS，传入闭包：

   ```python
   for input, target in dataset:
       def closure():
           optimizer.zero_grad()
           output = model(input)
           loss = loss_fn(output, target)
           loss.backward()
           return loss
       optimizer.step(closure)
   ```

   

3. `add_param_group()`：**添加参数组**。就是添加到`self.param_groups`中，**当前模型有多组参数组时使用。**

4. `state_dict()`：获取优化器当前状态信息字典。

   ```python
   def state_dict(self):
           r"""Returns the state of the optimizer as a :class:`dict`.
   
           It contains two entries:
   
           * state - a dict holding current optimization state. Its content
               differs between optimizer classes.
           * param_groups - a dict containing all parameter groups
           """
           # Save order indices instead of Tensors
           param_mappings = {}
           start_index = 0
   
           def pack_group(group):
               nonlocal start_index
               packed = {k: v for k, v in group.items() if k != 'params'}
               param_mappings.update({id(p): i for i, p in enumerate(group['params'], start_index)
                                      if id(p) not in param_mappings})
               packed['params'] = [param_mappings[id(p)] for p in group['params']]
               start_index += len(packed['params'])
               return packed
           param_groups = [pack_group(g) for g in self.param_groups]
           # Remap state to use order indices as keys
           packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v
                           for k, v in self.state.items()}
           return {
               'state': packed_state,
               'param_groups': param_groups,
           }
   ```

   返回的就是**参数组及各个参数组的信息**，**记录优化器的状态，方便掉电后回复训练。**

5. `load_state_dict()`：加载状态信息字典。



## 二、`torch.nn.optim.SGD`—随机梯度下降

```python
torch.optim.SGD(
    params, 
    lr=<required parameter>, 
    momentum=0, 
    dampening=0, 
    weight_decay=0, 
    nesterov=False)
```

Parameters

- **params** (*iterable*) – iterable of parameters to optimize or dicts defining parameter groups
- **lr** ([*float*](https://docs.python.org/3/library/functions.html#float)) – learning rate
- **momentum** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – momentum factor (default: 0)
- **weight_decay** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – weight decay (L2 penalty) (default: 0)
- **dampening** ([*float*](https://docs.python.org/3/library/functions.html#float)*,* *optional*) – dampening for momentum (default: 0)
- **nesterov** ([*bool*](https://docs.python.org/3/library/functions.html#bool)*,* *optional*) – enables Nesterov momentum (default: False)

带动量(Momentum)的梯度下降：
$$
v_i=m\times v_{i-1}+g(w_i)\\
w_{i+1}=w_i-lr\times v_i
$$
带动量的梯度下降就是**结合了当前的梯度和上一次更新的信息来进行当前更新**，可以加快训练速度。

> 还有很多优化器，看文档即可。



