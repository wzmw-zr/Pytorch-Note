# 基本的网络层

## 一、卷积层(convolution layer)

1. **卷积运算**：卷积核在输入信号(图像)上滑动，相应位置上进行乘加(因为深度学习的卷积中数字是随机的，所以不用管对称之类的。)
2. **卷积核(convolution kernel)**：卷积核又称为滤波器，过滤器，可认为是某种模式，某种特征。
3. 卷积的过程类似于用**一个模式去图像上寻找与它相似的区域**，与卷积核模式越相似，激活值越高，从而实现**特征提取**，例如边缘、条纹、色彩等细节模式。
4. **卷积维度**：卷积核在几个维度上滑动，就是几维卷积，图片一般是二维卷积。

### 1.`torch.nn.Conv2d`—二维卷积

```python
class Conv2d(_ConvNd):
     def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: _size_2_t,
        stride: _size_2_t = 1,
        padding: _size_2_t = 0,
        dilation: _size_2_t = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = 'zeros'  # TODO: refine this type
    ):
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        super(Conv2d, self).__init__(
            in_channels, out_channels, kernel_size, stride, padding, dilation,
            False, _pair(0), groups, bias, padding_mode)
```

参数解释：

+ in_channels：输入通道数。
+ out_channels：输出通道数，等价于卷积核个数。
+ kernel_size： 卷积核尺寸。
+ stride：步长。
+ padding：填充个数。
+ dilation：扩张卷积大小(也叫做空洞卷积)。
+ group:分组卷积设置。
+ bias：偏置。
+ padding_mode：填充模式。

输出卷积的尺寸计算公式：
$$
H_{out}=\lfloor\frac{H_{in}+2\times padding-dilation\times(kernel\_size-1)-1}{stride}\rfloor+1
$$


### 2.`torch.nn.ConvTranspose2d`—转置卷积

1. 转置卷积又称为部分跨越卷积(Fractionally strided Convolution) ，用于**对图像进行上采样**(UpSample)。

2. 假设图像尺寸$4\times4$，卷积核$3\times3$，padding=0, stride=1

   **正常卷积**：

   图像:$I_{16\times1}$   卷积核:$K_{4\times16}$    输出：$O_{4\times1}=K_{4\times16}\times I_{16\times1}$。

   假设图像尺寸为$2\times2$，卷积核$3\times3$，padding=0, stride = 1

   **转置卷积**：

   图像:$I_{4\times1}$    卷积核:$K_{16\times4}$     输出:$O_{16\times1}=K_{16\times4}\times I_{4\times1}$。



```python
class ConvTranspose2d(_ConvTransposeNd):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: _size_2_t,
        stride: _size_2_t = 1,
        padding: _size_2_t = 0,
        output_padding: _size_2_t = 0,
        groups: int = 1,
        bias: bool = True,
        dilation: int = 1,
        padding_mode: str = 'zeros'
    ):
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        output_padding = _pair(output_padding)
        super(ConvTranspose2d, self).__init__(
            in_channels, out_channels, kernel_size, stride, padding, dilation,
            True, output_padding, groups, bias, padding_mode)
```

参数解释与普通卷积一样。

转置卷积输出尺寸的计算公式：
$$
H_{out}=(H_{in}-1)\times stride - 2\times padding+dilation\times(kernel\_size-1)\\+output_padding+1
$$
**==卷积和转置卷积不互为逆运算！！！==**



## 二、池化层(Pooling Layer)

1. **池化运算**：对信号进行“**收集**”并“**总结**”，类似水池收集水资源，因而称为池化层。 
   + **收集**：多变少。
   + **总结**：最大值、平均值或其他。
2. 池化可以实现冗余信息的剔除，减少后面的计算量。



### (1) `torch.nn.MaxPool2d`类—最大池化

```python
class MaxPool2d(_MaxPoolNd):
     def __init__(self, 
                  kernel_size: _size_any_t, 
                  stride: Optional[_size_any_t] = None,
                  padding: _size_any_t = 0, 
                  dilation: _size_any_t = 1,
                  return_indices: bool = False, 
                  ceil_mode: bool = False) -> None:
        super(_MaxPoolNd, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride if (stride is not None) else kernel_size
        self.padding = padding
        self.dilation = dilation
        self.return_indices = return_indices
        self.ceil_mode = ceil_mode
```

参数解释：

+ kernel_size: 池化核尺寸。
+ stride：步长。
+ padding：填充个数。
+ dilation：池化核间隔大小。
+ ceil_mode：尺寸向上取整。
+ **return_indices：记录池化像素索引。==这个在一些图像分割任务中的最大值反池化上采样阶段会用到。==**



### (2)`torch.nn.AvgPool2d`—平均池化

```python
class AvgPool2d(_AvgPoolNd):
	def __init__(self, 
                 kernel_size: _size_2_t, 
                 stride: Optional[_size_2_t] = None, 
                 padding: _size_2_t = 0,
                 ceil_mode: bool = False, 
                 count_include_pad: bool = True, 
                 divisor_override: bool = None) -> None:
        super(AvgPool2d, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride if (stride is not None) else kernel_size
        self.padding = padding
        self.ceil_mode = ceil_mode
        self.count_include_pad = count_include_pad
        self.divisor_override = divisor_override
```

参数解释：

+ kernel_size: 池化核尺寸。
+ stride：步长。
+ padding：填充个数。
+ ceil_mode：尺寸向上取整。
+ count_include_pad：填充值用于计算。
+ divisor_override: 除法因子。默认是卷积核中元素个数，但是可以自定义。



### (3) `torch.nn.MaxUnpool2d`—最大值反池化上采样

```python
class MaxUnpool2d(_MaxUnpoolNd):
    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = None, padding: _size_2_t = 0) -> None:
        super(MaxUnpool2d, self).__init__()
        self.kernel_size = _pair(kernel_size)
        self.stride = _pair(stride if (stride is not None) else kernel_size)
        self.padding = _pair(padding)
        
    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
        return F.max_unpool2d(input, indices, self.kernel_size, self.stride,
                              self.padding, output_size)
```

**最大值反池化上采样的前向传播forward时传入的的indices是最大值池化的的最大像素索引。**

>还有很多池化层看文档即可。



## 三、线性层(Linear Layer)—全连接层

1. 线性层又称全连接层，其每个神经元与上一层所有神经元相连，实现对前一层的**线性组合，线性变换，对应于矩阵乘法。**

### (1) `torch.nn.Linear`—线性层、全连接层

```python
class Linear(Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:
        super(Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
```

参数解释：

+ in_feautres: 输入的特征数(节点数)。
+ out_features:输出的特征数(节点数)。
+ bias：偏置。

> 文档里还有另外两种线性层。



## 四、激活函数层(Activation Layer)

1. 激活函数对特征进行非线性变换，赋予多层神经网络具有深度的意义。

   > 如果没有激活函数，那么多层全连接层就相当于一层全连接层，根据矩阵乘法的结合律可得。

   常用的激活函数有Sigmod，Tanh，ReLU(以及对ReLu负半轴斜率调整的LeakyReLU，PReLU，RReLU)，softmax等。



## 五、其他网络层

实际上，还有用于正则化的Normalization Layer， Dropout Layer，用于编码解码的Transformer Layers，还有Sparse Layers，Recurrent Layers等网络层。