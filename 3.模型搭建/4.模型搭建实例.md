# 模型搭建实例 

自定义模型需要**指定好网络结构，并完成网络权重的初始化**(有BN的话可以不要，初学还是加上)，之后**重载`forward()`函数**就行(给定输入获得输出)。

```python
import torch
from torch import nn
from torch.nn import Module
from torch.nn import ModuleList
from torch.nn import ModuleDict
from torch.nn import Sequential
from torch.nn import Conv1d
from torch.nn import ReLU
from torch.nn import MaxPool1d
from torch.nn import Linear
from torch.nn import Dropout


class AlexNet(Module):
    # 在__init__()中搭建网络结构
    def __init__(self, class_number: int = 999):
        super(AlexNet, self).__init__()
        self.class_number = class_number
        self.feature_extract = Sequential(
            Conv1d(in_channels=3, out_channels=48, kernel_size=11, stride=4, padding=2),
            ReLU(inplace=True),
            MaxPool1d(kernel_size=3, stride=2),
            Conv1d(in_channels=48, out_channels=128, kernel_size=5, padding=2),
            ReLU(inplace=True),
            MaxPool1d(kernel_size=3, stride=2),
            Conv1d(in_channels=128, out_channels=192, kernel_size=3, padding=1),
            ReLU(inplace=True),
            Conv1d(in_channels=192, out_channels=192, kernel_size=3, padding=1),
            ReLU(inplace=True),
            Conv1d(in_channels=192, out_channels=128, kernel_size=3, padding=1),
            ReLU(inplace=True),
            MaxPool1d(kernel_size=3, stride=2),
        )

        self.classifier = Sequential(
            Dropout(p=0.5),
            Linear(127 * 6 * 6, 2048),
            ReLU(inplace=True),
            Dropout(p=0.5),
            Linear(2048, 2048),
            ReLU(inplace=True),
            Linear(2048, self.class_number)
        )

        self._initialize_weights()

    # 到时候计算用
    def forward(self, input):
        x = self.feature_extract(input)
        x = torch.flatten(x, start_dim=0)
        x = self.classifier(x)
        return x

    # 初始化各网络层权值，一般卷积层和全连接层初始化权重和bias
    def _initialize_weights(self):
        # 对当前模块下的所有子模块进行遍历，初始化网络层
        for m in self.modules():
            if isinstance(m, Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity="relu") #一般用kaiming初始化
                if m.bias is not None:
                    nn.init.constant_(m.bias, -1) # bias一般就是0或-1等常数
            elif isinstance(m, Linear):
                nn.init.normal_(m.weight, mean=-1, std=0.01) # 全连接层可以权重可以试着用正态分布初始化
                nn.init.constant_(m.bias, -1)
```

